---
title: "title"
author: "Neil Stein and Mitch Bobbin"
date: "date"
format: 
  pdf:
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
output:
  echo: false
  eval: false
---

**Due 11/9 at 5:00PM Central. Worth 100 points + 10 points extra credit.**

## Submission Steps (10 pts)
1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person *Partner 1*.
    - Partner 1 (Mitch Bobbin mbobbin):
    - Partner 2 (Neil Stein neilstein):
3. Partner 1 will accept the `ps5` and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted. 
4. "This submission is our work alone and complies with the 30538 integrity policy." Add your initials to indicate your agreement: \*\*\_\_\*\* \*\*\_\_\*\*
5. "I have uploaded the names of anyone else other than my partner and I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**"  (1 point)
6. Late coins used this pset: \*\*\_\_\*\* Late coins left after submission: \*\*\_\_\*\*
7. Knit your `ps5.qmd` to an PDF file to make `ps5.pdf`, 
    * The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate. 
8. (Partner 1): push  `ps5.qmd` and `ps5.pdf` to your github repo.
9. (Partner 1): submit `ps5.pdf` via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescope

\newpage

```{python}
import pandas as pd
import altair as alt
import time

import warnings 
warnings.filterwarnings('ignore')
alt.renderers.enable("png")
```


## Step 1: Develop initial scraper and crawler

### 1. Scraping (PARTNER 1)

```{python}
import pandas as pd
from bs4 import BeautifulSoup
import requests

# Define the URL
url = "https://oig.hhs.gov/fraud/enforcement/"

# Send a GET request to the URL
response = requests.get(url)

# Parse the HTML content with BeautifulSoup
soup = BeautifulSoup(response.text, "html.parser")

# Find all h2 tags for enforcement action titles
tag_h = soup.find_all("h2", class_="usa-card__heading")

# Find all span tags for dates
span_dates = soup.find_all("span", class_="text-base-dark padding-right-105")

# Find all ul tags for categories
ul_categories = soup.find_all("ul", class_="display-inline add-list-reset")

# Extract text and hyperlinks
enforcement = []
dates = []
categories = []
hyperlinks = []

for h2, span, ul in zip(tag_h, span_dates, ul_categories):
    # Extract the hyperlink and text from h2
    a_tag = h2.find("a")
    if a_tag:
        enforcement.append(a_tag.text.strip())
        hyperlinks.append(a_tag['href'])  # Extract the hyperlink
    
    # Extract the date
    dates.append(span.text.strip())
    
    # Extract the category text
    category_text = [li.text.strip() for li in ul.find_all("li")]
    categories.append(", ".join(category_text))  # Join multiple categories if present

# Create a DataFrame
df = pd.DataFrame({
    "Title of Enforcement Action": enforcement,
    "Date": dates,
    "Category": categories,
    "Hyperlink": hyperlinks
})

# Display the DataFrame
print(df)
```

  
### 2. Crawling (PARTNER 1)

```{python}
import requests
from bs4 import BeautifulSoup

tag_h = soup.find_all("h2", class_="usa-card__heading")

my_links=[]
for h2 in tag_h:
  links=h2.find_all("a", href=True)
  for link in links:
    my_links.append(
      "https://oig.hhs.gov"+link.get("href"))

agency = []

for url in my_links:
    response = requests.get(url)
    page_soup = BeautifulSoup(response.text, "html.parser")
    li_tags = page_soup.find_all("li")
    
    for li in li_tags:
        # Find the `span` with the "Agency:" label
        label_span = li.find("span", class_="padding-right-2 text-base")
        # Check if it contains the text "Agency:"
        if label_span and "Agency:" in label_span.text:
            agency_text = label_span.next_sibling
            if agency_text:
                agency_name = agency_text.strip() #Remove whitespace
                agency.append(agency_name)  #Append to the list

print("Final agency list:", agency)

df["agency"]=agency

print(df.head())

```

## Step 2: Making the scraper dynamic

### 1. Turning the scraper into a function 

* a. Pseudo-Code (PARTNER 2)

  - writing a prompt using the input() method as the first step in function
  - that value is then converted to a datetime object and compared to the 2013 restriction listed. If/else statements should cover for this need
  - with that input datetime, we can use it as the key value in a for loop that covers every day between that date until today() as the limit. This loop will process (ascending) every full day (yyyy-mm-dd)
  - 1 second time delay (to avoid being blocked by the site) will be implemented using a 1 second delay with time.sleep()

* b. Create Dynamic Scraper (PARTNER 2)

```{python}
import time
import requests
from bs4 import BeautifulSoup

def scraper_function():
  # input date step - confirming correct format
  date_input = input("Please enter a year & month (format should be [yyyy-mm]): ")
  try:
    date_obj = pd.to_datetime(date_input)
    # return date_obj
  except ValueError:
    print("Invalid date format. Please ony use YYYY-MM format.")
  # checking the date is within given range 
  cutoff_date = pd.to_datetime("2013-01-01")
  if date_obj >= cutoff_date:
    for date_obj in pd.date_range(start= date_obj, end= pd.to_datetime("today")):
        formatted_date = date_obj.strftime("%Y-%m")
        page_number = 1
        while True:
          # Define the URL
          scrape_url = "https://oig.hhs.gov/fraud/enforcement/?page={page_number}"

          # Send a GET request to the URL
          scrape_response = requests.get(scrape_url)
          # Parse the HTML content with BeautifulSoup
          scrape_soup = BeautifulSoup(scrape_response.text, "html.parser")

          # Find all h2 tags for enforcement action titles
          scrape_tag_h = scrape_soup.find_all("h2", class_= "usa-card__heading")

          # Find all span tags for dates
          scrape_span_dates = scrape_soup.find_all("span", class_= "text-base-dark padding-right-105")

          # Find all ul tags for categories
          scrape_ul_categories = scrape_soup.find_all("ul", class_="display-inline add-list-reset")

          # Extract text and hyperlinks
          scrape_enforcement = []
          scrape_dates = []
          scrape_categories = []
          scrape_hyperlinks = []

          for h2, span, ul in zip(scrape_tag_h, scrape_span_dates, scrape_ul_categories):
              # Extract the hyperlink and text from h2
              a_tag = h2.find("a")
              if a_tag:
                  enforcement.append(a_tag.text.strip())
                  hyperlinks.append(a_tag['href'])  # Extract the hyperlink
              
              # Extract the date
              scrape_dates.append(span.text.strip())
              
              # checking that our date is still within range
              latest_date = pd.to_datetime(scrape_dates[-1])
              if latest_date <= date_obj:
                break

              # Extract the category text
              scrape_category_text = [li.text.strip() for li in ul.find_all("li")]
              scrape_categories.append(", ".join(category_text))  # Join multiple categories if present

        # Create a DataFrame
          scrape_df = pd.DataFrame({
              "Title of Enforcement Action": scrape_enforcement,
              "Date": scrape_dates,
              "Category": scrape_categories,
              "Hyperlink": scrape_hyperlinks,
        })
    # Implementing a 1 second delay
    page_number += 1
    time.sleep(1)
  else:
    print("Please enter a date after the cutoff date:", cutoff_date.strftime("%Y-%m"))
  scrape_df.to_csv("scraped_data.csv", index= False)

# Call the scraper function
scraper_function()

```



* c. Test Partner's Code (PARTNER 1)

```{python}

```

## Step 3: Plot data based on scraped data

### 1. Plot the number of enforcement actions over time (PARTNER 2)

```{python}

```

### 2. Plot the number of enforcement actions categorized: (PARTNER 1)

* based on "Criminal and Civil Actions" vs. "State Enforcement Agencies"

```{python}

```

* based on five topics

```{python}

```

## Step 4: Create maps of enforcement activity

### 1. Map by State (PARTNER 1)

```{python}

```


### 2. Map by District (PARTNER 2)

```{python}

```

## Extra Credit

### 1. Merge zip code shapefile with population
```{python}

```

### 2. Conduct spatial join
```{python}

```

### 3. Map the action ratio in each district
```{python}

```