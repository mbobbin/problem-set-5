---
title: "title"
author: "Neil Stein and Mitch Bobbin"
date: "date"
format: 
  pdf:
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
output:
  echo: false
  eval: false
---

**Due 11/9 at 5:00PM Central. Worth 100 points + 10 points extra credit.**

## Submission Steps (10 pts)
1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person *Partner 1*.
    - Partner 1 (Mitch Bobbin mbobbin):
    - Partner 2 (Neil Stein neilstein):
3. Partner 1 will accept the `ps5` and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted. 
4. "This submission is our work alone and complies with the 30538 integrity policy." Add your initials to indicate your agreement: \*\*\_\_\*\* \*\*\_\_\*\*
5. "I have uploaded the names of anyone else other than my partner and I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**"  (1 point)
6. Late coins used this pset: \*\*\_\_\*\* Late coins left after submission: \*\*\_\_\*\*
7. Knit your `ps5.qmd` to an PDF file to make `ps5.pdf`, 
    * The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate. 
8. (Partner 1): push  `ps5.qmd` and `ps5.pdf` to your github repo.
9. (Partner 1): submit `ps5.pdf` via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescope

\newpage

```{python}
import pandas as pd
import altair as alt
import time

import warnings 
warnings.filterwarnings('ignore')
alt.renderers.enable("png")
```


## Step 1: Develop initial scraper and crawler

### 1. Scraping (PARTNER 1)

```{python}
import pandas as pd
from bs4 import BeautifulSoup
import requests

# Define the URL
url = "https://oig.hhs.gov/fraud/enforcement/"

# Send a GET request to the URL
response = requests.get(url)

# Parse the HTML content with BeautifulSoup
soup = BeautifulSoup(response.text, "html.parser")

# Find all h2 tags for enforcement action titles
tag_h = soup.find_all("h2", class_="usa-card__heading")

# Find all span tags for dates
span_dates = soup.find_all("span", class_="text-base-dark padding-right-105")

# Find all ul tags for categories
ul_categories = soup.find_all("ul", class_="display-inline add-list-reset")

# Extract text and hyperlinks
enforcement = []
dates = []
categories = []
hyperlinks = []

for h2, span, ul in zip(tag_h, span_dates, ul_categories):
    # Extract the hyperlink and text from h2
    a_tag = h2.find("a")
    if a_tag:
        enforcement.append(a_tag.text.strip())
        hyperlinks.append(a_tag['href'])  # Extract the hyperlink
    
    # Extract the date
    dates.append(span.text.strip())
    
    # Extract the category text
    category_text = [li.text.strip() for li in ul.find_all("li")]
    categories.append(", ".join(category_text))  # Join multiple categories if present

# Create a DataFrame
df = pd.DataFrame({
    "Title of Enforcement Action": enforcement,
    "Date": dates,
    "Category": categories,
    "Hyperlink": hyperlinks
})

# Display the DataFrame
print(df)
```

  
### 2. Crawling (PARTNER 1)

```{python}
import requests
from bs4 import BeautifulSoup

tag_h = soup.find_all("h2", class_="usa-card__heading")

my_links=[]
for h2 in tag_h:
  links=h2.find_all("a", href=True)
  for link in links:
    my_links.append(
      "https://oig.hhs.gov"+link.get("href"))

agency = []

for url in my_links:
    response = requests.get(url)
    page_soup = BeautifulSoup(response.text, "html.parser")
    li_tags = page_soup.find_all("li")
    found_agency=False
    for li in li_tags:
        # Find the `span` with the "Agency:" label
        label_span = li.find("span", class_="padding-right-2 text-base")
        # Check if it contains the text "Agency:"
        if label_span and "Agency:" in label_span.text:
            agency_text = label_span.next_sibling
            if agency_text:
                agency_name = agency_text.strip() #Remove whitespace
                agency.append(agency_name)  #Append to the list
            else:
                agency.append("NA")  # Append "NA" if agency name is missing
            found_agency = True
            break  # Stop checking other `li` tags if agency is found

    if not found_agency:
        # Append "NA" if no "Agency:" label was found in any `li` tag
        agency.append("NA")


df["agency"]=agency

print(df.head())

```

## Step 2: Making the scraper dynamic

### 1. Turning the scraper into a function 

* a. Pseudo-Code (PARTNER 2)

  - writing a prompt using the input() method as the first step in function
  - that value is then converted to a datetime object and compared to the 2013 restriction listed. If/else statements should cover for this need
  - with that input datetime, we can use it as the key value in a for loop that covers every day between that date until today() as the limit. This loop will process (ascending) every full day (yyyy-mm-dd)
  - 1 second time delay (to avoid being blocked by the site) will be implemented using a 1 second delay with time.sleep()

* b. Create Dynamic Scraper (PARTNER 2)

```{python}
import pandas as pd
import requests
import time
from bs4 import BeautifulSoup

def scraper_function():
    # Input date step - confirming correct format
    date_input = input("Please enter a year & month (format should be [yyyy-mm]): ")
    try:
        date_obj = pd.to_datetime(date_input)
    except ValueError:
        print("Invalid date format. Please use YYYY-MM format.")
        return  # Exit the function if the date format is invalid
    
    # Checking the date is within the given range 
    cutoff_date = pd.to_datetime("2013-01-01")
    if date_obj < cutoff_date:
        print("Please enter a date after the cutoff date:", cutoff_date.strftime("%Y-%m"))
        return  # Exit if the date is before the cutoff
    
    # Loop over each month from the input date to the current date
    all_data = []  # Collect data for all months and pages
    for month_date in pd.date_range(start=date_obj, end=pd.to_datetime("today"), freq='MS'):
        formatted_date = month_date.strftime("%Y-%m")
        page_number = 1
        print(f"Scraping data for {formatted_date}...")

        while True:
            # Define the URL with page number
            scrape_url = f"https://oig.hhs.gov/fraud/enforcement/?page={page_number}"
            scrape_response = requests.get(scrape_url)

            if scrape_response.status_code != 200:
                print(f"Error fetching page {page_number} for {formatted_date}. Skipping...")
                break  # Exit the loop if the page is not found or request fails

            # Parse the HTML content with BeautifulSoup
            scrape_soup = BeautifulSoup(scrape_response.text, "html.parser")

            # Find all required elements
            scrape_tag_h = scrape_soup.find_all("h2", class_="usa-card__heading")
            scrape_span_dates = scrape_soup.find_all("span", class_="text-base-dark padding-right-105")
            scrape_ul_categories = scrape_soup.find_all("ul", class_="display-inline add-list-reset")

            # Break if no data is found on the page
            if not scrape_tag_h:
                break

            # Extract text and hyperlinks, ensuring each list has a matching entry
            for h2, span, ul in zip(scrape_tag_h, scrape_span_dates, scrape_ul_categories):
                # Extract the hyperlink and text from h2
                title = h2.find("a").text.strip() if h2.find("a") else "N/A"
                hyperlink = "https://oig.hhs.gov" + h2.find("a")['href'] if h2.find("a") else "N/A"
                
                # Extract the date
                date_text = span.text.strip() if span else "N/A"
                latest_date = pd.to_datetime(date_text, errors='coerce')
                
                # Check if the latest date is within range and stop if it's before the target date
                if latest_date and latest_date < date_obj:
                    print("Reached date outside range. Stopping early.")
                    scrape_df = pd.DataFrame(all_data, columns=["Title of Enforcement Action", "Date", "Category", "Hyperlink"])
                    scrape_df.to_csv("scraped_data.csv", index=False)
                    print("Data has been saved to 'scraped_data.csv'.")
                    return  # Stop the function if date is outside range
                
                # Extract the category text
                category_text = [li.text.strip() for li in ul.find_all("li")] if ul else ["N/A"]
                categories = ", ".join(category_text)

                # Append to the all_data list as a tuple
                all_data.append((title, date_text, categories, hyperlink))

            # Move to the next page and add a delay
            page_number += 1
            time.sleep(0.5)  # Reduce delay if possible for faster scraping

    # Save the data at the end of all months
    scrape_df = pd.DataFrame(all_data, columns=["Title of Enforcement Action", "Date", "Category", "Hyperlink"])
    scrape_df.to_csv("scraped_data.csv", index=False)
    print("Data has been saved to 'scraped_data.csv'.")

# Call the scraper function
scraper_function()
```

* c. Test Partner's Code (PARTNER 1)

```{python}
scraped_data_df=pd.read_csv("scraped_data.csv")
print(scraped_data_df.iloc[-1])
```

We have 3,022 actions in our final df. Note that this can change by the time we submit the assignment, as the site is updated regularly and I am writing this answer on 11/8/24.

Date of earliest scraped enforcement action: Jan 4th, 2021. The title of the action was The United States And Tennessee Resolve Claims With Three Providers For False Claims Act Liability Relating To ‘P-Stim’ Devices For A Total Of $1.72 Million. It was in the Criminal and Civil Actions Category. The URL is https://oig.hhs.gov/fraud/enforcement/the-united-states-and-tennessee-resolve-claims-with-three-providers-for-false-claims-act-liability-relating-to-p-stim-devices-for-a-total-of-172-million/

## Step 3: Plot data based on scraped data

### 1. Plot the number of enforcement actions over time (PARTNER 2)

```{python}
import pandas as pd
import altair as alt

# filtering towards the prompt's specified date range
clipping_start_date = pd.to_datetime("2021-01-01")
clipping_end_date = pd.to_datetime("today")
scraped_data_df["Date"] = pd.to_datetime(scraped_data_df["Date"])

plotting_enforcement_df = scraped_data_df[(scraped_data_df["Date"] >= clipping_start_date) & (scraped_data_df["Date"] <= clipping_end_date)]

# aggregating over month & year
grouped_enforcement = plotting_enforcement_df.groupby(pd.Grouper(key= "Date", freq= "M")).size().reset_index(name= "Count")

# line chart plotting
enforc_chart = alt.Chart(grouped_enforcement).mark_line().encode(
    x= alt.X("Date:N", title= "Date (month)"),
    y= alt.Y("Count", title= "Enforcement Actions Count"),
).properties(
    title= "Chart of Enforcement Actions Over Time"
).interactive()

enforc_chart.show()
```

### 2. Plot the number of enforcement actions categorized: (PARTNER 1)

* based on "Criminal and Civil Actions" vs. "State Enforcement Agencies"

```{python}
import altair as alt
#Create a chart that uses different shapes for "Criminal & Civil"
#and the State Enforcement Agency. Then use a str detect to #further classify based upon the title of the enforcement action.

alt.Chart(scraped_data_df).transform_filter(
        (alt.datum.Category == "State Enforcement Agencies") | 
        (alt.datum.Category == "Criminal and Civil Actions")).mark_line().encode(
  alt.X("yearmonth(Date):T"),
  alt.Y("count()"),
  color=("Category")
)
```

* based on five topics

```{python}
import numpy as np
crim_civil_df=scraped_data_df[scraped_data_df["Category"]=="Criminal and Civil Actions"]

#Define a function to determine the topic based on keywords
def assign_topic(title):
    title = title.lower()
    if "healthcare" in title or "health care" in title or "medic" in title or "claims" in title:
        return "Health Care Fraud"
    elif "bank" in title or "financial" in title:
        return "Financial Fraud"
    elif "drug" in title or "distribut" in title:
        return "Drug Enforcement"
    elif "bribe" in title or "corruption" in title or "kickback" in title:
      return "Bribery/Corruption"
    else:
        return "Other"

crim_civil_df["Topic"] = crim_civil_df["Title of Enforcement Action"].apply(assign_topic)


alt.Chart(crim_civil_df).mark_line().encode(
  alt.X("yearmonth(Date):T"),
  alt.Y("count()"),
  color="Topic"
).properties(title="Monthly HHS Criminal & Civil Actions, 2021-Present")

```

## Step 4: Create maps of enforcement activity

### 1. Map by State (PARTNER 1)

```{python}
#apply the for loop to extract each rows agency:
first_30=scraped_data_df.head(50)
import requests
from bs4 import BeautifulSoup
import time



agency = []

for url in scraped_data_df["Hyperlink"]:
    try:
        # Measure the time taken for each request
        request_start_time = time.time()
        
        # Send a GET request to the URL
        response = requests.get(url)
        
        request_end_time = time.time()
        print(f"Time taken for request to {url}: {request_end_time - request_start_time} seconds")

        # Parse the HTML content
        page_soup = BeautifulSoup(response.text, "html.parser")
        ul_tags=page_soup.find_all("ul")
        
        for ul in ul_tags:
          li_tags = ul.find_all("li")
        found_agency = False
            for li in li_tags:
               # Measure time taken to check each `li` tag
               li_start_time = time.time()
            
            # Find the span with the "Agency:" label
              label_span = li.find("span", class_="padding-right-2 text-base")
            
            if label_span and "Agency:" in label_span.text:
                agency_text = label_span.next_sibling
                if agency_text:
                    agency_name = agency_text.strip()  # Remove whitespace
                    agency.append(agency_name)  # Append to the list
                else:
                    agency.append("NA")  # Append "NA" if agency name is missing
                found_agency = True
                break  # Stop checking other `li` tags if agency is found
            
            li_end_time = time.time()
            print(f"Time taken to check an `li` tag: {li_end_time - li_start_time} seconds")

        if not found_agency:
            # Append "NA" if no "Agency:" label was found in any `li` tag
            agency.append("NA")
        
        # Optional delay to avoid overwhelming the server
        time.sleep(.5)

    except requests.RequestException as e:
        # Handle request-related errors
        print(f"Error fetching {url}: {e}")
        agency.append("NA")

# Add the agency information to the DataFrame
scraped_data_df["agency"] = agency


```

```{python}

```

### 2. Map by District (PARTNER 2)

```{python}

```

## Extra Credit

### 1. Merge zip code shapefile with population
```{python}

```

### 2. Conduct spatial join
```{python}

```

### 3. Map the action ratio in each district
```{python}

```