---
title: "title"
author: "author"
date: "date"
format: 
  pdf:
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
output:
  echo: false
  eval: false
---

**Due 11/9 at 5:00PM Central. Worth 100 points + 10 points extra credit.**

## Submission Steps (10 pts)
1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person *Partner 1*.
    - Partner 1 (Mitch Bobbin mbobbin):
    - Partner 2 (Neil Stein neilstein):
3. Partner 1 will accept the `ps5` and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted. 
4. "This submission is our work alone and complies with the 30538 integrity policy." Add your initials to indicate your agreement: \*\*\_\_\*\* \*\*\_\_\*\*
5. "I have uploaded the names of anyone else other than my partner and I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**"  (1 point)
6. Late coins used this pset: \*\*\_\_\*\* Late coins left after submission: \*\*\_\_\*\*
7. Knit your `ps5.qmd` to an PDF file to make `ps5.pdf`, 
    * The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate. 
8. (Partner 1): push  `ps5.qmd` and `ps5.pdf` to your github repo.
9. (Partner 1): submit `ps5.pdf` via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescope

\newpage

```{python}
import pandas as pd
import altair as alt
import time

import warnings 
warnings.filterwarnings('ignore')
alt.renderers.enable("png")
```


## Step 1: Develop initial scraper and crawler

### 1. Scraping (PARTNER 1)

```{python}
import pandas as pd
from bs4 import BeautifulSoup
import requests

# Define the URL
url = "https://oig.hhs.gov/fraud/enforcement/"

# Send a GET request to the URL
response = requests.get(url)

# Parse the HTML content with BeautifulSoup
soup = BeautifulSoup(response.text, "html.parser")

# Find all h2 tags for enforcement action titles
tag_h = soup.find_all("h2", class_="usa-card__heading")

# Find all span tags for dates
span_dates = soup.find_all("span", class_="text-base-dark padding-right-105")

# Find all ul tags for categories
ul_categories = soup.find_all("ul", class_="display-inline add-list-reset")

# Extract text and hyperlinks
enforcement = []
dates = []
categories = []
hyperlinks = []

for h2, span, ul in zip(tag_h, span_dates, ul_categories):
    # Extract the hyperlink and text from h2
    a_tag = h2.find("a")
    if a_tag:
        enforcement.append(a_tag.text.strip())
        hyperlinks.append(a_tag['href'])  # Extract the hyperlink
    
    # Extract the date
    dates.append(span.text.strip())
    
    # Extract the category text
    category_text = [li.text.strip() for li in ul.find_all("li")]
    categories.append(", ".join(category_text))  # Join multiple categories if present

# Create a DataFrame
df = pd.DataFrame({
    "Title of Enforcement Action": enforcement,
    "Date": dates,
    "Category": categories,
    "Hyperlink": hyperlinks
})

# Display the DataFrame
print(df)
```

  
### 2. Crawling (PARTNER 1)

```{python}
import requests
from bs4 import BeautifulSoup

# Assuming `my_links` has been populated with the correct URLs
tag_h = soup.find_all("h2", class_="usa-card__heading")

my_links=[]
for h2 in tag_h:
  links=h2.find_all("a", href=True)
  for link in links:
    my_links.append(
      "https://oig.hhs.gov"+link.get("href"))

agency = []

for url in my_links:
    response = requests.get(url)
    page_soup = BeautifulSoup(response.text, "html.parser")
    li_tags = page_soup.find_all("li")
    
    for li in li_tags:
        # Find the `span` with the "Agency:" label
        label_span = li.find("span", class_="padding-right-2 text-base")
        
        # Debug: Check if we found the `span` with the correct class
        if label_span:
            print("Found span:", label_span.text)  # Check the text content of the `span`
        
        # Check if it contains the text "Agency:"
        if label_span and "Agency:" in label_span.text:
            # The agency name should be the next sibling text of the `span`
            agency_text = label_span.next_sibling
            if agency_text:
                agency_name = agency_text.strip()
                print("Found agency:", agency_name)  # Debug: print the agency name found
                agency.append(agency_name)  # Append to the list

print("Final agency list:", agency)

df["agency"]=agency

```

## Step 2: Making the scraper dynamic

### 1. Turning the scraper into a function 

* a. Pseudo-Code (PARTNER 2)


* b. Create Dynamic Scraper (PARTNER 2)

```{python}

```

* c. Test Partner's Code (PARTNER 1)

```{python}

```

## Step 3: Plot data based on scraped data

### 1. Plot the number of enforcement actions over time (PARTNER 2)

```{python}

```

### 2. Plot the number of enforcement actions categorized: (PARTNER 1)

* based on "Criminal and Civil Actions" vs. "State Enforcement Agencies"

```{python}

```

* based on five topics

```{python}

```

## Step 4: Create maps of enforcement activity

### 1. Map by State (PARTNER 1)

```{python}

```


### 2. Map by District (PARTNER 2)

```{python}

```

## Extra Credit

### 1. Merge zip code shapefile with population
```{python}

```

### 2. Conduct spatial join
```{python}

```

### 3. Map the action ratio in each district
```{python}

```