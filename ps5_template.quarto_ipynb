{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"30538 Problem Set 5: Web Scraping\"\n",
        "author: \"Neil Stein and Mitch Bobbin\"\n",
        "date: \"Nov. 11\"\n",
        "format: \n",
        "  pdf:\n",
        "    include-in-header: \n",
        "       text: |\n",
        "         \\usepackage{fvextra}\n",
        "         \\DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\\\\{\\}}\n",
        "include-before-body:\n",
        "  text: |\n",
        "    \\RecustomVerbatimEnvironment{verbatim}{Verbatim}{\n",
        "      showspaces = false,\n",
        "      showtabs = false,\n",
        "      breaksymbolleft={},\n",
        "      breaklines\n",
        "    }\n",
        "output:\n",
        "  echo: false\n",
        "  eval: false\n",
        "---\n",
        "\n",
        "\n",
        "**Due 11/9 at 5:00PM Central. Worth 100 points + 10 points extra credit.**\n",
        "\n",
        "## Submission Steps (10 pts)\n",
        "1. This problem set is a paired problem set.\n",
        "2. Play paper, scissors, rock to determine who goes first. Call that person *Partner 1*.\n",
        "    - Partner 1 (Mitch Bobbin mbobbin):\n",
        "    - Partner 2 (Neil Stein neilstein):\n",
        "3. Partner 1 will accept the `ps5` and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted. \n",
        "4. \"This submission is our work alone and complies with the 30538 integrity policy.\" Add your initials to indicate your agreement: \\*\\*\\_\\_\\*\\* \\*\\*\\_\\_\\*\\*\n",
        "5. \"I have uploaded the names of anyone else other than my partner and I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**\"  (1 point)\n",
        "6. Late coins used this pset: \\*\\*\\_\\_\\*\\* Late coins left after submission: \\*\\*\\_\\_\\*\\*\n",
        "7. Knit your `ps5.qmd` to an PDF file to make `ps5.pdf`, \n",
        "    * The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate. \n",
        "8. (Partner 1): push  `ps5.qmd` and `ps5.pdf` to your github repo.\n",
        "9. (Partner 1): submit `ps5.pdf` via Gradescope. Add your partner on Gradescope.\n",
        "10. (Partner 1): tag your submission in Gradescope\n",
        "\n",
        "\\newpage\n"
      ],
      "id": "84e77bbd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import altair as alt\n",
        "import time\n",
        "\n",
        "import warnings \n",
        "warnings.filterwarnings('ignore')\n",
        "alt.renderers.enable(\"png\")"
      ],
      "id": "dbd1ae8b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Develop initial scraper and crawler\n",
        "\n",
        "### 1. Scraping (PARTNER 1)\n"
      ],
      "id": "57a274fc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "url = \"https://oig.hhs.gov/fraud/enforcement/\"\n",
        "\n",
        "response = requests.get(url)\n",
        "\n",
        "#parse the HTML content\n",
        "soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "#find all h2 tags for enforcement action titles\n",
        "tag_h = soup.find_all(\"h2\", class_=\"usa-card__heading\")\n",
        "\n",
        "#find all span tags for dates\n",
        "span_dates = soup.find_all(\"span\", class_=\"text-base-dark padding-right-105\")\n",
        "\n",
        "#find all ul tags for categories\n",
        "ul_categories = soup.find_all(\"ul\", class_=\"display-inline add-list-reset\")\n",
        "\n",
        "#create empty lists to store our extracted data in\n",
        "enforcement = []\n",
        "dates = []\n",
        "categories = []\n",
        "hyperlinks = []\n",
        "\n",
        "#loop over the objects we created from finding tags\n",
        "for h2, span, ul in zip(tag_h, span_dates, ul_categories):\n",
        "    #extract the hyperlink and text from h2\n",
        "    a_tag = h2.find(\"a\")\n",
        "    if a_tag:\n",
        "        enforcement.append(a_tag.text.strip())\n",
        "        hyperlinks.append(a_tag['href'])  \n",
        "        #extract the hyperlink\n",
        "    \n",
        "    #extract the date and strip whitespace\n",
        "    dates.append(span.text.strip())\n",
        "    \n",
        "    #extract the category text\n",
        "    category_text = [li.text.strip() for li in ul.find_all(\"li\")]\n",
        "    categories.append(\", \".join(category_text))  \n",
        "    #join multiple categories if present\n",
        "\n",
        "#create the df\n",
        "df = pd.DataFrame({\n",
        "    \"Title of Enforcement Action\": enforcement,\n",
        "    \"Date\": dates,\n",
        "    \"Category\": categories,\n",
        "    \"Hyperlink\": hyperlinks\n",
        "})\n",
        "\n",
        "print(df)"
      ],
      "id": "b2b484db",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Crawling (PARTNER 1)\n"
      ],
      "id": "5d47c445"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "tag_h = soup.find_all(\"h2\", class_=\"usa-card__heading\")\n",
        "#get the hyperlinks and store them in a list\n",
        "my_links=[]\n",
        "for h2 in tag_h:\n",
        "  links=h2.find_all(\"a\", href=True)\n",
        "  for link in links:\n",
        "    my_links.append(\n",
        "      \"https://oig.hhs.gov\"+link.get(\"href\"))\n",
        "\n",
        "#create empty list to store all the hyperlinks into\n",
        "agency = []\n",
        "#loop over all the urls in the mylinks list to find the \n",
        "#agency\n",
        "for url in my_links:\n",
        "    response = requests.get(url)\n",
        "    page_soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "    li_tags = page_soup.find_all(\"li\")\n",
        "    found_agency=False\n",
        "    #loop over the li tags that we found\n",
        "    for li in li_tags:\n",
        "        #find span labels\n",
        "        label_span = li.find(\"span\", class_=\"padding-right-2 text-base\")\n",
        "        #see if text has Agency in it\n",
        "        if label_span and \"Agency:\" in label_span.text:\n",
        "            agency_text = label_span.next_sibling\n",
        "            if agency_text:\n",
        "                agency_name = agency_text.strip() #remove #whitespace\n",
        "                agency.append(agency_name)  #append to the list\n",
        "            else:\n",
        "                agency.append(\"NA\")  #if agency na\n",
        "            found_agency = True\n",
        "            break  #break if we find the agency\n",
        "\n",
        "    if not found_agency:\n",
        "        #append \"NA\" if we don't find an agency\n",
        "        agency.append(\"NA\")\n",
        "\n",
        "\n",
        "df[\"agency\"]=agency\n",
        "\n",
        "print(df.head())"
      ],
      "id": "1955454c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Making the scraper dynamic\n",
        "\n",
        "### 1. Turning the scraper into a function \n",
        "\n",
        "* a. Pseudo-Code (PARTNER 2)\n",
        "\n",
        "  - writing a prompt using the input() method as the first step in function\n",
        "  - that value is then converted to a datetime object and compared to the 2013 restriction listed. If/else statements should cover for this need\n",
        "  - with that input datetime, we can use it as the key value in a for loop that covers every day between that date until today() as the limit. This loop will process (ascending) every full day (yyyy-mm-dd)\n",
        "  - 1 second time delay (to avoid being blocked by the site) will be implemented using a 1 second delay with time.sleep()\n",
        "\n",
        "* b. Create Dynamic Scraper (PARTNER 2)\n"
      ],
      "id": "b856665d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "import pandas as pd\n",
        "import requests\n",
        "import time\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def scraper_function():\n",
        "    #input date step and confirm correct format\n",
        "    date_input = input(\"Please enter a year & month (format should be [yyyy-mm]): \")\n",
        "    try:\n",
        "        date_obj = pd.to_datetime(date_input)\n",
        "    except ValueError:\n",
        "        print(\"Invalid date format. Please use YYYY-MM format.\")\n",
        "        return  #exit the function if the date format is invalid\n",
        "    \n",
        "    #checking the date is within the given range \n",
        "    cutoff_date = pd.to_datetime(\"2013-01-01\")\n",
        "    if date_obj < cutoff_date:\n",
        "        print(\"Please enter a date after the cutoff date:\", cutoff_date.strftime(\"%Y-%m\"))\n",
        "        return  #exit if the date is before the cutoff\n",
        "    \n",
        "    #coop over each month from the input date to the current date\n",
        "    all_data = []  #collect data for all months and pages\n",
        "    for month_date in pd.date_range(start=date_obj, end=pd.to_datetime(\"today\"), freq='MS'):\n",
        "        formatted_date = month_date.strftime(\"%Y-%m\")\n",
        "        page_number = 1\n",
        "        print(f\"Scraping data for {formatted_date}...\")\n",
        "\n",
        "        while True:\n",
        "            #define the URL with page number\n",
        "            scrape_url = f\"https://oig.hhs.gov/fraud/enforcement/?page={page_number}\"\n",
        "            scrape_response = requests.get(scrape_url)\n",
        "\n",
        "            if scrape_response.status_code != 200:\n",
        "                print(f\"Error fetching page {page_number} for {formatted_date}. Skipping...\")\n",
        "                break  #exit the loop if the page is not found #or request fails\n",
        "\n",
        "            #parse the HTML content with BeautifulSoup\n",
        "            scrape_soup = BeautifulSoup(scrape_response.text, \"html.parser\")\n",
        "\n",
        "            #find all required elements\n",
        "            scrape_tag_h = scrape_soup.find_all(\"h2\", class_=\"usa-card__heading\")\n",
        "            scrape_span_dates = scrape_soup.find_all(\"span\", class_=\"text-base-dark padding-right-105\")\n",
        "            scrape_ul_categories = scrape_soup.find_all(\"ul\", class_=\"display-inline add-list-reset\")\n",
        "\n",
        "            #break if no data is found on the page\n",
        "            if not scrape_tag_h:\n",
        "                break\n",
        "\n",
        "            #extract text and hyperlinks, ensuring each list #has a matching entry\n",
        "            for h2, span, ul in zip(scrape_tag_h, scrape_span_dates, scrape_ul_categories):\n",
        "                #extract the hyperlink and text from h2\n",
        "                title = h2.find(\"a\").text.strip() if h2.find(\"a\") else \"N/A\"\n",
        "                hyperlink = \"https://oig.hhs.gov\" + h2.find(\"a\")['href'] if h2.find(\"a\") else \"N/A\"\n",
        "                \n",
        "                #extract the date\n",
        "                date_text = span.text.strip() if span else \"N/A\"\n",
        "                latest_date = pd.to_datetime(date_text, errors='coerce')\n",
        "                \n",
        "                #check if the latest date is within range and stop if it's before the target date\n",
        "                if latest_date and latest_date < date_obj:\n",
        "                    print(\"Reached date outside range. Stopping early.\")\n",
        "                    scrape_df = pd.DataFrame(all_data, columns=[\"Title of Enforcement Action\", \"Date\", \"Category\", \"Hyperlink\"])\n",
        "                    scrape_df.to_csv(\"scraped_data.csv\", index=False)\n",
        "                    print(\"Data has been saved to 'scraped_data.csv'.\")\n",
        "                    return  #stop the function if date is \n",
        "                    #outside range\n",
        "                \n",
        "                # Extract the category text\n",
        "                category_text = [li.text.strip() for li in ul.find_all(\"li\")] if ul else [\"N/A\"]\n",
        "                categories = \", \".join(category_text)\n",
        "\n",
        "                #append to the all_data list as a tuple\n",
        "                all_data.append((title, date_text, categories, hyperlink))\n",
        "\n",
        "            #move to the next page and add a delay\n",
        "            page_number += 1\n",
        "            time.sleep(0.5)\n",
        "\n",
        "    #save the data at the end of all months\n",
        "    scrape_df = pd.DataFrame(all_data, columns=[\"Title of Enforcement Action\", \"Date\", \"Category\", \"Hyperlink\"])\n",
        "    scrape_df.to_csv(\"scraped_data.csv\", index=False)\n",
        "    print(\"Data has been saved to 'scraped_data.csv'.\")\n",
        "\n",
        "scraper_function()"
      ],
      "id": "69ad9a41",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* c. Test Partner's Code (PARTNER 1)\n"
      ],
      "id": "a42f37f8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "scraper_function()"
      ],
      "id": "ffcc04e8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "scraped_data_df=pd.read_csv(\"scraped_data.csv\")\n",
        "print(scraped_data_df.iloc[-1])"
      ],
      "id": "0be15420",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We have 3,022 actions in our final df. Note that this can change by the time we submit the assignment, as the site is updated regularly and I am writing this answer on 11/8/24.\n",
        "\n",
        "Date of earliest scraped enforcement action: Jan 4th, 2021. The title of the action was The United States And Tennessee Resolve Claims With Three Providers For False Claims Act Liability Relating To ‘P-Stim’ Devices For A Total Of $1.72 Million. It was in the Criminal and Civil Actions Category. The URL is https://oig.hhs.gov/fraud/enforcement/the-united-states-and-tennessee-resolve-claims-with-three-providers-for-false-claims-act-liability-relating-to-p-stim-devices-for-a-total-of-172-million/\n",
        "\n",
        "## Step 3: Plot data based on scraped data\n",
        "\n",
        "### 1. Plot the number of enforcement actions over time (PARTNER 2)\n"
      ],
      "id": "31dc38e3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import altair as alt\n",
        "\n",
        "# filtering towards the prompt's specified date range\n",
        "clipping_start_date = pd.to_datetime(\"2021-01-01\")\n",
        "clipping_end_date = pd.to_datetime(\"today\")\n",
        "scraped_data_df[\"Date\"] = pd.to_datetime(scraped_data_df[\"Date\"])\n",
        "\n",
        "plotting_enforcement_df = scraped_data_df[(scraped_data_df[\"Date\"] >= clipping_start_date) & (scraped_data_df[\"Date\"] <= clipping_end_date)]\n",
        "\n",
        "# aggregating over month & year\n",
        "grouped_enforcement = plotting_enforcement_df.groupby(pd.Grouper(key= \"Date\", freq= \"M\")).size().reset_index(name= \"Count\")\n",
        "\n",
        "# line chart plotting\n",
        "enforc_chart = alt.Chart(grouped_enforcement).mark_line().encode(\n",
        "    x= alt.X(\"Date:T\", title= \"Date (month-year)\"),\n",
        "    y= alt.Y(\"Count\", title= \"Enforcement Actions Count\"),\n",
        ").properties(\n",
        "    title= \"Chart of Enforcement Actions Over Time\",\n",
        "    width= 600,\n",
        "    height= 400,\n",
        ").interactive()\n",
        "\n",
        "enforc_chart.show()"
      ],
      "id": "47055984",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Plot the number of enforcement actions categorized: (PARTNER 1)\n",
        "\n",
        "* based on \"Criminal and Civil Actions\" vs. \"State Enforcement Agencies\"\n"
      ],
      "id": "49834736"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import altair as alt\n",
        "\n",
        "#filter out anything other than state or crim and civil from\n",
        "#our category variable. Then color based upon the two \n",
        "#dif categories:\n",
        "state_v_crim_plot=alt.Chart(scraped_data_df).transform_filter(\n",
        "        (\n",
        "            alt.datum.Category == \"State Enforcement Agencies\") | \n",
        "        (\n",
        "            alt.datum.Category == \"Criminal and Civil Actions\")\n",
        "            ).mark_line().encode(\n",
        "  alt.X(\"yearmonth(Date):T\"),\n",
        "  alt.Y(\"count()\"),\n",
        "  color=(\"Category\")\n",
        ").properties(\n",
        "    title=\"HHS Actions by Enforcement Level, 2021-Present\")\n",
        "state_v_crim_plot.show()"
      ],
      "id": "27e1d982",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* based on five topics\n"
      ],
      "id": "f60b6e19"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "#filter\n",
        "crim_civil_df=scraped_data_df[scraped_data_df[\"Category\"]==\"Criminal and Civil Actions\"]\n",
        "\n",
        "#Define a function to determine the topic based on keywords\n",
        "def assign_topic(title):\n",
        "    title = title.lower()\n",
        "    if \"healthcare\" in title or \"health care\" in title or \"medic\" in title or \"claims\" in title:\n",
        "        return \"Health Care Fraud\"\n",
        "    elif \"bank\" in title or \"financial\" in title:\n",
        "        return \"Financial Fraud\"\n",
        "    elif \"drug\" in title or \"distribut\" in title:\n",
        "        return \"Drug Enforcement\"\n",
        "    elif \"bribe\" in title or \"corruption\" in title or \"kickback\" in title:\n",
        "      return \"Bribery/Corruption\"\n",
        "    else:\n",
        "        return \"Other\"\n",
        "\n",
        "#apply the function to the df's enfrocement action, and assign\n",
        "#the result to a new column\n",
        "crim_civil_df[\"Topic\"] = crim_civil_df[\"Title of Enforcement Action\"].apply(assign_topic)\n",
        "\n",
        "#plot with the topic being the color of the line\n",
        "topic_chart=alt.Chart(crim_civil_df).mark_line().encode(\n",
        "  alt.X(\"yearmonth(Date):T\"),\n",
        "  alt.Y(\"count()\"),\n",
        "  color=\"Topic\"\n",
        ").properties(title=\"Monthly HHS Criminal & Civil Actions, 2021-Present\")\n",
        "topic_chart.show()"
      ],
      "id": "0ea9faef",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Create maps of enforcement activity\n",
        "\n",
        "### 1. Map by State (PARTNER 1)\n"
      ],
      "id": "79db8886"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "#apply the for loop to extract each rows agency:\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "\n",
        "agency=[]\n",
        "\n",
        "\n",
        "for url in scraped_data_df[\"Hyperlink\"]:\n",
        "    try:\n",
        "        #get start time. ultimately will use for sanity check\n",
        "        #while it is running.\n",
        "        request_start_time = time.time()\n",
        "        \n",
        "        #get request for each url that is present in hyperlink\n",
        "        #column.\n",
        "        response = requests.get(url)\n",
        "        #end time to see how long it takes to get the url\n",
        "        request_end_time = time.time()\n",
        "        print(f\"Time taken for request to {url}: {request_end_time - request_start_time} seconds\")\n",
        "\n",
        "        #parse the HTML content\n",
        "        page_soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "        \n",
        "        #find ul tags that are relevant\n",
        "        ul_tags = page_soup.find_all(\"ul\", class_=\"usa-list usa-list--unstyled margin-y-2\")\n",
        "        #set a variabl that can tell us if we found an agency\n",
        "        #the default will be false.\n",
        "        found_agency = False\n",
        "        for ul in ul_tags:\n",
        "            #loop over all the li tags in the ul tags\n",
        "            li_tags = ul.find_all(\"li\")\n",
        "            \n",
        "            for li in li_tags:\n",
        "                #measure how long its taking to find the li tags\n",
        "                li_start_time = time.time()\n",
        "                \n",
        "                #find the span with the Agency: label \n",
        "                label_span = li.find(\"span\", class_=\"padding-right-2 text-base\")\n",
        "                \n",
        "                if label_span and \"Agency:\" in label_span.text:\n",
        "                    agency_text = label_span.next_sibling\n",
        "                    if agency_text:\n",
        "                        agency_name = agency_text.strip()  #remove whitespace\n",
        "                        agency.append(agency_name)  \n",
        "                        #append to the list\n",
        "                    else:\n",
        "                        agency.append(\"NA\")  \n",
        "                        #append \"NA\" if agency name is missing.\n",
        "                        #change the found agency value to true\n",
        "                        #to exit the loop\n",
        "                    found_agency = True\n",
        "                    break  #stop checking other li tags if agency is found\n",
        "                \n",
        "                li_end_time = time.time()\n",
        "                print(f\"Time taken to check an `li` tag: {li_end_time - li_start_time} seconds\")\n",
        "\n",
        "            if found_agency:\n",
        "                break  \n",
        "            #exit the outer loop once the agency is found\n",
        "\n",
        "        if not found_agency:\n",
        "            #append \"NA\" if we dont find an agency\n",
        "            agency.append(\"NA\")\n",
        "\n",
        "        time.sleep(.5)\n",
        "\n",
        "    except requests.RequestException as e:\n",
        "        #handle request-related errors\n",
        "        print(f\"Error fetching {url}: {e}\")\n",
        "        agency.append(\"NA\")\n",
        "\n",
        "#add the agency information to the df\n",
        "scraped_data_df[\"agency\"] = agency"
      ],
      "id": "bb716ce4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import geopandas as gpd\n",
        "import numpy as np\n",
        "\n",
        "state_geo_data= gpd.read_file(\"cb_2018_us_state_500k.shp\")\n",
        "\n",
        "state_actions_df=scraped_data_df[scraped_data_df[\"Category\"]==\"State Enforcement Agencies\"]\n",
        "\n",
        "#clean the agency column so we can extract the state. remove\n",
        "#any of the strings that aren't a state:\n",
        "state_actions_df[\"state\"] = np.where(\n",
        "    state_actions_df[\"agency\"].notnull(),\n",
        "    state_actions_df[\"agency\"]\n",
        "    .str.replace(r\"(State of |State |General|Genera|Inspector|U\\.S\\.|Attorney|Northern|Eastern|Western|Southern|Office,|Department|of|Justice|District|['’`]s\\s*)\", \"\", regex=True)\n",
        "    .str.strip(),  #remove any leading or trailing whitespace\n",
        "    np.nan  #assign NaN if the agency column is empty\n",
        ")\n",
        "\n",
        "#change a few names since there's some cleaning still to be done\n",
        "#and the format should match the geo df for the merge\n",
        "state_actions_df[\"state\"] = np.where(\n",
        "    state_actions_df[\"state\"].str.contains(\"California\", case=False, na=False),\n",
        "    \"California\",\n",
        "    state_actions_df[\"state\"]\n",
        ")\n",
        "\n",
        "state_actions_df[\"state\"] = np.where(\n",
        "    state_actions_df[\"state\"].str.contains(\"Hawai’i\", case=False, na=False),\n",
        "    \"Hawaii\",\n",
        "    state_actions_df[\"state\"]\n",
        ")\n",
        "\n",
        "state_actions_df[\"state\"] = np.where(\n",
        "    state_actions_df[\"state\"].str.contains(\"Virgin Islands\", case=False, na=False),\n",
        "    \"United States Virgin Islands\",\n",
        "    state_actions_df[\"state\"]\n",
        ")\n",
        "\n",
        "#now groupby and give us a count for each state in our data\n",
        "state_actions_totals_df=state_actions_df.groupby(\"state\").size().reset_index(name=\"Count\")\n",
        "#rename the column in geo df to match our state actions df\n",
        "state_geo_data = state_geo_data.rename(columns={\"NAME\": \"state\"})\n",
        "#merge df with gdf\n",
        "state_geo_data_totals = state_geo_data.merge(\n",
        "    state_actions_totals_df, on=\"state\", how=\"left\")\n",
        "\n",
        "#fill any states who had no obs in our original df with 0s    \n",
        "state_geo_data_totals[\"Count\"] = state_geo_data_totals[\"Count\"].fillna(0)\n",
        "\n",
        "#plot map, making sure to provide borders to each state,\n",
        "#and changing the color scheme so its more clear which states\n",
        "#have high values and which have low.\n",
        "alt.Chart(state_geo_data_totals).mark_geoshape(\n",
        "    stroke=\"black\", strokeWidth=.5).encode(\n",
        "    alt.X(\"geometry\"),\n",
        "    color=alt.Color(\"Count\",scale=alt.Scale(range=[\"white\",\"darkblue\"]))\n",
        ").project(\"albersUsa\"\n",
        ").properties(title=\"Number of HHS State Enforcement Actions by State, 2021-Present\")"
      ],
      "id": "55823c0a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Map by District (PARTNER 2)\n"
      ],
      "id": "2d8266cc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import geopandas as gpd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# importing our state shapefile\n",
        "us_states = gpd.read_file(\"cb_2018_us_state_500k.shp\")\n",
        "\n",
        "# processing our agency column into districts (and states) using a Regular Expressions function\n",
        "# got the idea from stack overflow - https://stackoverflow.com/questions/55194224/extract-names-from-string-with-python-regex\n",
        "\n",
        "district_df = scraped_data_df.copy()\n",
        "district_df[\"agency\"] = district_df[\"agency\"].astype(str)\n",
        "\n",
        "\n",
        "# empty columns to be filled\n",
        "district_df[\"State\"]= \"\"\n",
        "district_df[\"District\"] = \"\"\n",
        "\n",
        "# regular expression function\n",
        "def state_district_extractor(string):\n",
        "  pattern = r\"U\\.S\\. Attorney's Office(?:, (Eastern|Western|Northern|Middle|Central|Southern|District of) )?(.*)\"\n",
        "\n",
        "  match = re.match(pattern, string)\n",
        "  # distict name case\n",
        "  if match:\n",
        "    district = match.group(1) + \" \" + match.group(2) if match.group(1) else match.group(2)\n",
        "    state = match.group(2)\n",
        "    return state, district\n",
        "\n",
        "  # \"state of\" name case\n",
        "  elif \"State of\" in string or not string.split():\n",
        "    state = string.split(\"State of \")[1].strip()\n",
        "    return state, None\n",
        "\n",
        "  # just state name case\n",
        "  elif string.strip():\n",
        "    state = string.strip()\n",
        "    return state, None\n",
        "\n",
        "  # multi-agency cases\n",
        "  elif \"U.S. Department of Justice\" in string:\n",
        "    state = string.split(\"District of \")[-1].strip()\n",
        "    return state, \"N/A\"\n",
        "\n",
        "  elif \"U.S. Attorney's Office\" in string:\n",
        "      remaining_part = string.split(\"U.S. Attorney's Office, \")[1].strip()\n",
        "      parts = remaining_part.split(\"District of \")\n",
        "      state = parts[-1].strip()\n",
        "      district = parts[0].strip()\n",
        "      return state, district\n",
        "\n",
        "  # empty cases\n",
        "  return \"N/A\", \"N/A\"\n",
        "\n",
        "district_df[[\"State\",\"District\"]] = district_df[\"agency\"].apply(state_district_extractor).apply(pd.Series)\n",
        "\n",
        "# Correcting for Washington, DC and other exceptions\n",
        "district_df.loc[district_df[\"State\"] == \"Columbia\", \"State\"] = \"Washington, D.C.\"\n",
        "district_df.loc[district_df[\"State\"] == \"Columbia\", \"State\"] = \"Washington, D.C.\"\n",
        "\n",
        "district_df.loc[district_df[\"District\"].isnull() & district_df[\"agency\"].str.contains(\"U.S. Attorney's Office\"), \"District\"] = district_df[\"agency\"].str.replace(\"U.S. Attorney's Office, \", '', regex=False)\n",
        "# cleanup in case the terminology makes it through\n",
        "district_df[\"State\"] = district_df[\"State\"].str.replace(\"District of \", \"\", regex= False)\n",
        "district_df[\"State\"] = district_df[\"State\"].str.replace(\"U.S. Attorney’s Office, \", \"\", regex= False)\n",
        "district_df[\"State\"] = district_df[\"State\"].str.replace(\"Attorney General\", \"\", regex= False)\n",
        "district_df[\"State\"] = district_df[\"State\"].str.replace(\"U.S. Attorney General\", \"\", regex= False)\n",
        "district_df[\"State\"] = district_df[\"State\"].str.replace(\"U.S. Department of Justice and U.S. Attorney's Office, \", \"\", regex= False)\n",
        "# persistant cases of the direction -- specific exceptions for states with directional names\n",
        "district_df[\"State\"] = district_df[\"State\"].str.replace(\"Northern\", \"\", regex= False)\n",
        "district_df[\"State\"] = district_df[\"State\"].str.replace(\"Eastern\", \"\", regex= False)\n",
        "district_df[\"State\"] = district_df[\"State\"].str.replace(\"Southern\", \"\", regex= False)\n",
        "district_df[\"State\"] = district_df[\"State\"].str.replace(\"Western\", \"\", regex= False)\n",
        "district_df[\"State\"] = district_df[\"State\"].str.replace(\"Middle\", \"\", regex= False)\n",
        "district_df[\"State\"] = district_df[\"State\"].str.replace(\"Central\", \"\", regex= False)\n",
        "\n",
        "# choropleth mapping by district\n",
        "district_grouping = district_df.groupby(\"State\").size().reset_index(name= \"District_Count\")\n",
        "merged_df = us_states.merge(district_grouping, left_on= \"NAME\", right_on= \"State\", how= \"left\")\n",
        "# per the recommendation in Ed, I will use the Albers USA projection\n",
        "merged_df = merged_df.to_crs(\"ESRI:102003\")\n",
        "\n",
        "# plotting\n",
        "merged_df.plot(column= \"District_Count\", cmap= \"OrRd\", figsize=(10, 8))\n",
        "plt.title(\"Number of HHS Enforcement Actions by Districts per State\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "id": "76ae4752",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extra Credit\n",
        "\n",
        "### 1. Merge zip code shapefile with population"
      ],
      "id": "1c527f9c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Conduct spatial join"
      ],
      "id": "f188d337"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Map the action ratio in each district"
      ],
      "id": "a7cf574a"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/neilstein/Library/Python/3.12/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}